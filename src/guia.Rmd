---
title: "Optimización numérica"
author: Juan David Ospina Arango <br> Redes Neuronales y Algoritmos Bioinspirados
  <br> Universidad Nacional de Colombia <br> Facultad de Minas
date: "Semestre 2025-01"
output:
  html_document:
    toc: true
  pdf_document:
    toc: true
---

```{r}
knitr::opts_chunk$set(echo=TRUE)
library(GA)
```

# Ilustración de la optimización con Algoritmos Evolutivos

## Rastrigin

### 1. Programación de la función

```{r}

f_rastrigin <- function(x) {
  A <- 10
  n <- length(x)
  z <- sum(x^2 - A * cos(2*pi*x))
  
  return(z)
}
```

Hagamos una prueba de escritorio

```{r}
set.seed(1983)
x_0 <- runif(5, min=-5.12, max=5.12)
f_rastrigin(x_0)
```

Adaptado de [Dive into Deep Learning](https://d2l.ai/index.html) capítulo 11.

# Optimización de Rastrigin

```{r}
fitness_rastrigin <- function(x) {
  return(-f_rastrigin(x))
}
```

```{r}

optim_ga_rastrigin <- ga(type="real-valued", 
                         fitness = fitness_rastrigin,
                         lower = c(-5.12, -5.12),
                         upper = c(5.12, 5.12),
                         seed = 1983
                         )
```

La solución es

Veamos la trayectoria del mejor valor

```{r}
plot(optim_ga_rastrigin)
```

```{r}
print(optim_ga_rastrigin@solution)
```

## Optimización basada en la derivada

Cuando se conoce la derivada o el gradiente de una función objetivo $f$, esta información se puede usar para encontrar un mínimo cercano al punto $x_0$. Para esto se puede usar un procedimiento iterativo.

Si $i$ indica el número de iteración, entonces la regla de actualización es $x^{(i)} = x^{(i-1)} - \eta \nabla f(x^{(i-1)})$, con $x^{(0)}=x_0$. El parámetro $\eta$ se conoce como la tasa de aprendizaje.

La siguiente función implementa en `R` el método de descenso por gradiente en problemas univariados:

```{r}
optimizador_basico <- function(fun,grad_fun,x0,eta=0.01,max_eval=100){
  # fun: es la función que se desea optimizar (no se utiliza)
  # grad_fun: es una función que permite calcular el gradiente de fun
  # x0: es la condición inicial
  # eta: es la tasa de aprendizaje
  # max_eval: es el máximo número de iteraciones 
  x <- rep(NA,max_eval)
  x[1] <- x0
  for (i in 2:max_eval){
    x[i] <- x[i-1]-eta*grad_fun(x[i-1])
    cambio_opt <- abs(x[i-1]-x[i])
    if (cambio_opt<0.00001){ # si el cambio en la solución es menor a este valor
                            # termina el proceso de optimización
      break
    }
  }
  return(x[1:i])
}
```

Prueba de escritorio del optimizador con la función $f(x)=x^2$, $x \in \mathbb{R}$:

```{r}
tray <- optimizador_basico(fun=\(x) x^2,grad_fun=\(x) 2*x,x0=0.2,eta=0.1,max_eval = 1000)
plot(tray,las=1,xlab="i",ylab = expression(x^(i)),
     main="Valor de la solución vs iteración")
```

Ahora probemos con una función un poco más compleja. Consideremos la función $f(x)=x \cos(x)$, cuya derivada es $f'(x)=-x \sin(x) + \cos(x)$. El siguiente código implementa tanto la función como su derivada:

```{r}
f <- function(x){
  y <- x*cos(x)
  return(y)
}

grad_f <- function(x){
  y <- -x*sin(x)+cos(x)
}

```

Veamos la gráfica de la función $f(x)=x \cos(x)$ entre 0 y 10:

```{r}
curve(f,0,10,las=1,main=expression('f(x)=x cos(x)'),lwd=2)
grid()
```

Ahora utilicemos el optimizador para optimizar la función $f(x)=x \cos(x)$:

```{r}
tray <- optimizador_basico(fun=f,grad_fun=grad_f,x0=6.7,eta=0.1,max_eval = 1000)
curve(f,0,10,las=1, lwd=2)
y <- f(tray)
lines(tray,y, type="p",cex=1.5,col="red")
grid()
```

Una pregunta importante es cuál es el impacto de la condición inicial y de la tasa de aprendizaje. Variar estos parámetros en el código anterior permite aprender cosas interesantes sobre sus efectos.

### Cálculo de la derivada numérica

¿Qué pasa si la función es muy complicada para obtener una expresión de su derivada o su gradiente? En este caso una alternativa es usar una aproximación numérica de la derivada. Una aproximación usual es $f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}$, con $h$ pequeño.

El siguiente código muestra cómo calcular la derivada numérica:

```{r}
num_dev <- function(x,f,h=0.01){
  y <- (f(x+h)-f(x-h))/(2*h)
  return(y)
}
```

### Optimización univariada con la derivada numérica

El siguiente código muestra el proceso de optimización de descenso por gradiente (univariado) usando la derivada numérica:

<!-- el [gradiente numérico](https://en.wikipedia.org/wiki/Numerical_differentiation): -->

```{r}
optimizador_numdev <- function(fun,x0,eta=0.01,max_eval=100,h=0.01){
  # fun: es la función que se desea optimizar (no se utiliza)
  # x0: es la condición inicial
  # eta: es la tasa de aprendizaje
  # max_eval: es el máximo número de iteraciones
  # h: es el tamaño de ventana para el cálculo de la derivada numérica
  x <- rep(NA,max_eval)
  x[1] <- x0
  for (i in 2:max_eval){
    num_grad_fun <- num_dev(x[i-1],fun,h)
    x[i] <- x[i-1]-eta*num_grad_fun
    cambio_opt <- abs(x[i-1]-x[i])
    if (cambio_opt<0.00001){
      break
    }
  }
  return(x[1:i])
}
```

Prueba de escritorio de la optimización con derivada numérica usando la función $f(x)=x^2$, $x \in \mathbb{R}$:

```{r}
tray <- optimizador_numdev(fun=\(x) x^2,x0=0.2,eta=0.1,max_eval = 1000)
plot(tray,las=1,xlab="i",ylab = expression(x^(i)),
     main="Valor de la solución vs iteración")
```

Ahora probemos este algoritmo con la función $f(x)=x \cos(x)$:

```{r}
tray <- optimizador_numdev(fun=f,x0=6.1,eta=0.1,max_eval = 1000)
curve(f,0,10,las=1,lwd=2,main=expression('f(x)=x cos(x)'))
y <- f(tray)  # se evalúa x^(i) en la función
lines(tray,y, type="p",cex=1.5,col="red")
grid()
```

## Optimización multivariada

Pasemos ahora al contexto multivariado, es decir cuando $x \in \mathbb{R}^d$. Implementaremos un optimizador que use el gradiente numérico. Ahora, para obtener el gradiente numérico es necesario poder calcular la derivada parcial de la función con respecto a alguna de sus componentes.

$\frac{\partial f(x)}{\partial x_i} \approx \frac{f(x+he)-f(x-he)}{2h}$, con $h$ pequeño y $e$ es un vector de $d$ componentes que valen todas cero excepto la $i$-ésima que vale 1.

```{r}
partial_dev <- function(x,i,fun,h=0.01){
    e <- x*0 # crea un vector de ceros de la misma longitud de x
    e[i] <- h
    y <- (fun(x+e)-fun(x-e))/(2*h)
  return(y)
}
```

Para evaluar el gradiente de una función $f$ en $x$ hay que obtener cada una de las derivadas parciales de $f$ en $x$. Con el código anterior y la función `mapply()` se puede lograr esto fácilmente, como se muestra a continuación:

```{r}
num_grad <- function(x,fun,h=0.01){
  # x: punto del espacio donde se debe evaluar el gradiente
  # fun: función para la que se desea calcular el gradiente en x
  # h: es el tamaño de ventana para el cálculo de la derivada numérica
  d <- length(x)
  y <- mapply(FUN=partial_dev,i=1:d,MoreArgs=list(x=x,h=h,fun=fun))
  return(y)
}
```

Probemos el código anterior con la función $f(x_1, x_2)=x_1^2+x_2^2$, cuyo gradiente es $\nabla f = [2 x_1 \: 2 x_2 ]^T$. El siguiente código implementa la función:

```{r}
fun1 <- function(x){
  return(sum(x^2))
}
```

A continuación se calcula $\frac{\partial f}{\partial x_1}$ cuando $(x_1,x_2)^T=(1,1)$:

```{r}
partial_dev(c(1,1),1,fun=fun1)
```

Ahora se calcula $\nabla f$ cuando $(x_1,x_2)^T=(0.5,1)$:

```{r}
num_grad(x=c(0.5,1),fun=fun1)
```

### Cálculo numérico de la matriz hessiana

El siguiente paso en la optimización numérica es aprovechar la información que trae la matriz de segundas derivadas parciales o matriz hessiana $Hf(x)$. La $i$-ésima fila de esta matriz es $\frac{\partial}{\partial x_i} \nabla f^T$ (asumiento que el gradiente es un vector columna).

El siguiente código implementa el cálculo de $\frac{\partial}{\partial x_i} \nabla f^T$:

```{r}
deriv_grad <- function(x,fun,i=1,h=0.01){
  # x: punto en el que se evalúa el gradiente
  # fun: función para la cual se calcula la derivada del gradiente respecto a la íesima componente
  # i: i-ésima componente del vector x con respecto a la que se deriva
    e <- x*0 # crea un vector de ceros de la misma longitud de x
    e[i] <- h
    y <- (num_grad(x+e,fun=fun,h=h)-num_grad(x-e,fun=fun,h=h))/(2*h)
    return(y)
}
```

Ahora calculemos $\frac{\partial}{\partial x_2} \nabla f^T$ para $f(x_1, x_2)=x_1^2+x_2^2$ en $(x_1,x_2)^T=(0.5,1)$. El gradiente es $\nabla f^T = (2x_1,2x_2)$ y su derivada con respecto a $x_2$ es $\frac{\partial}{\partial x_2} \nabla f^T = (0,2)$ y no depende de $x_1$ y $x_2$:

```{r}
deriv_grad(x=c(0.5,1),fun=fun1,i=2)
```

Ya están disponibles los elementos para calcular la matriz hessiana. Nuevamente se usa la función `mapply` para derivar el gradiente con respecto a cada componente.

El siguiente código muestra cómo calcular numéricamente la matriz hessiana $Hf(x)$:

```{r}
matriz_hessiana <- function(x,fun,h=0.01){
  # x: punto en el que se evalúa la matriz hessiana
  # fun: función a la que se le calcula la matriz hessiana en x
  # h: es el tamaño de ventana para el cálculo de la derivada numérica
  d <- length(x)
  y <- mapply(FUN=deriv_grad,i=1:d,MoreArgs=list(x=x,h=h,fun=fun),SIMPLIFY = TRUE)
  return(y)
}

```

Ahora calculemos $Hf(x)$ para $f(x_1, x_2)=x_1^2+x_2^2$ en $(x_1,x_2)^T=(0.5,1)$. Las derivadas parciales del gradiente con respecto a $x_1$ y $x_2$ son $\frac{\partial}{\partial x_1}\nabla f^T = (2,0)$ y $\frac{\partial}{\partial x_2}\nabla f^T = (0,2)$ y no dependen de $x_1$ y $x_2$. La matriz hessiana no depende entonces de $x_1$ y $x_2$ y está dada por la siguiente expresión:

\begin{equation}
  Hf(x) = 
  \begin{pmatrix}
    \frac{\partial}{\partial x_1}\nabla f^T \\
    \frac{\partial}{\partial x_2}\nabla f^T
  \end{pmatrix}
  =
  \begin{pmatrix}
    2 & 0 \\
    0 & 2
  \end{pmatrix}
\end{equation}

Ahora hagamos el cálculo numérico con $(x_1,x_2)^T=(3,-1)$:

```{r}
matriz_hessiana(x=c(3,-1),fun=fun1)
```

<!-- Prueba de escritorio: -->

<!-- ```{r} -->

<!-- fun2 <- function(x){ -->

<!--   x1 <- x[1] -->

<!--   x2 <- x[2] -->

<!--   y <- 2*x1*x2+x1^2 -->

<!-- } -->

<!-- ``` -->

<!-- ```{r} -->

<!-- H <- matriz_hessiana(x=c(3,-1),fun=fun2) -->

<!-- print(H) -->

<!-- ``` -->

### Optimizador multivariado

Para la optimización multivariada se puede usar la regla de actualización $x^{(i)} = x^{(i-1)} - (Hf(x^{(i-1)}))^{-1} \nabla f^T(x^{(i-1)})$, con $x^{(0)}=x_0$. El siguiente código

```{r}
optimizador_mult_numdev <- function(x0,fun,max_eval=100,h=0.01,eta=0.01){
  x <- matrix(NA,ncol =length(x0), nrow = max_eval)
  x[1,] <- x0
  for (i in 2:max_eval){
    num_grad_fun <- num_grad(x[i-1,],fun,h)
    H <- matriz_hessiana(x[i-1,],fun,h)
    cambio <- - eta*solve(H)%*%num_grad_fun
    x[i,] <- x[i-1,] + cambio
    cambio_opt <- sqrt(sum((x[i-1,]-x[i,])^2))
    if (cambio_opt<0.00001){
      break
    }
  }
  return(x[1:i,])
}
```

Utilicemos la función `optimizador_numdev` para optimizar la función $f(x_1, x_2)=x_1^2+x_2^2$ con condición inicial $(x_1,x_2)^T=(1,5)$:

```{r}
sol <- optimizador_mult_numdev(fun1,x0=c(1,5),eta=1)
```

Veamos gráficamente la evolución de la solución:

```{r}
# Particionamiento del rango de cada variable
n_length <- 100
x1 <- seq(-8, 8, length.out = n_length)
x2 <- seq(-8, 8, length.out = n_length)
X <- expand.grid(x1,x2)
# Función f(x1,x2)=x1^2+x2^2
f1 <- function(x1,x2){
  return(x1^2+x2^2)
}
# Evaluación de la función:
z <- f1(X[,1],X[,2])
# Expresión del resultado como matriz para graficar:
Z <- matrix(z, ncol = n_length, nrow = n_length)
contour(x1, x2, Z, las=1,
        xlab = expression(x[1]), ylab = expression(x[2]),
        main = expression(f(x[1],x[2])==x[1]^2+x[2]^2),
        sub = "Curvas de nivel de la función")
lines(sol, type="b",cex=1.5,col="red")
```

Veamos qué pasa con la funció de Rastrigin definida como $f(x_1,x_2)=20+\sum_{i=1}^{2}{x_i^2-10 \cos(2 \pi x_i)}$, $x_i \in [-5.12,5.12]$, $i=1,2$. El siguiente código implementa la función:

```{r}
f_rastrigin2d <- function(x1,x2){
  # Esta versión de la función se usa para graficar.
  y <- 20+x1^2-10*cos(2*pi*x1)+x2^2-10*cos(2*pi*x2)
  return(y)
}

f_rastrigin2d_vec <- function(x){
  # Versión vectorizada. Es la que se utiliza para optimizar.
  x1 <- x[1]
  x2 <- x[2]
  y <- 20+x1^2-10*cos(2*pi*x1)+x2^2-10*cos(2*pi*x2)
  return(y)
}
```

Veamos ahora el proceso de optimización con condición inicial $(x_1,x_2)^T=(0.8,4.8)$:

```{r}
sol_ras <- optimizador_mult_numdev(f_rastrigin2d_vec,x0=c(0.8,4.8),eta=1)

n_length <- 150
x1 <- seq(-5.12,5.12,length.out=n_length)
x2 <- seq(-5.12,5.12,length.out=n_length)
X <- expand.grid(x1,x2)
z <- f_rastrigin2d(X[,1],X[,2])
Z <- matrix(z,ncol=n_length,nrow = n_length)
contour(x1, x2, Z, las=1,
        xlab = expression(x[1]), ylab = expression(x[2]),
        main = "Función de Rastrigin",
        sub = "Curvas de nivel de la función")
lines(sol_ras, type="b",cex=1.5,col="red")
```

Observemos el comportamiento de la optimización más de cerca:

```{r}
n_length <- 150
x1 <- seq(0,4,length.out=n_length)
x2 <- seq(4,6,length.out=n_length)
X <- expand.grid(x1,x2)
z <- f_rastrigin2d(X[,1],X[,2])
Z <- matrix(z,ncol=n_length,nrow = n_length)
contour(x1, x2, Z, las=1,
        xlab = expression(x[1]), ylab = expression(x[2]),
        main = "Función de Rastrigin",
        sub = "Curvas de nivel de la función")
lines(sol_ras, type="b",cex=1.5,col="red")
```

Esta estrategia de optimización no garantiza la convergencia al óptimo global. Además, depende fuertemente de la condición inicial. El conocimiento de la condición inicial refleja el conocimiento a priori sobre el problema en cuestión.

### Precondicionamiento

Se ha ilustrado el método de descenso por gradiente usando problemas en dos dimensiones. Una consecuencia de aumentar la dimensionalidad es que cada vez es más costoso obtener $(Hf(x^{(i)}))^{-1}$. Para un problema con $d$ variables, la matriz hessiana tiene $d^2$ componentes distintas que deben calcularse numéricamente. Una alternativa que permite usar información de la matriz hessiana y limitar el aumento del costo computacional es utilizar la regla $x^{(i)} = x^{(i-1)} - (diag(Hf(x^{(i)})))^{-1} \nabla f^T(x^{(i)})$, con $x^{(0)}=x_0$.

La matriz $diag(Hf(x^{(i)}))$ es una matriz diagonal cuyos elementos son la diagonal de $Hf(x^{(i)})$, o sea que solo tiene $d$ componentes distintos. Y para obtener su inversa se toma el recíproco de cada elemento de $Hf(x^{(i)})$, lo que simplifica los cálculos.

Derivadas de orden superior:

```{r}
deriv_segunda <- function(x,fun,i=1,h=0.01){
    e <- x*0 # crea un vector de ceros de la misma longitud de x
    e[i] <- h
    y <- (fun(x+e)-2*fun(x)+fun(x-e))/(h^2)
  return(y)
}
```

```{r}
optimizador_mult_precond <- function(x0,fun,max_eval=100,h=0.01,eta=0.01){
  x <- matrix(NA,ncol =length(x0), nrow = max_eval)
  d <- length(x0)
  x[1,] <- x0
  for (i in 2:max_eval){
    num_grad_fun <- num_grad(x[i-1,],fun,h)
    diag_H <- mapply(FUN=deriv_segunda,i=1:d,MoreArgs=list(x=x[i-1,],h=h,fun=fun))
    # print(diag_H)
    # H <- matriz_hessiana(x[i-1,],fun,h)
    # print(H)
    H_precond <- diag(1/diag_H)
    cambio <- - eta*H_precond%*%num_grad_fun
    x[i,] <- x[i-1,] + cambio
    cambio_opt <- sqrt(sum((x[i-1,]-x[i,])^2))
    if (cambio_opt<0.0000001){
      break
    }
  }
  return(x[1:i,])
}
```

```{r}
sol_ras_precon <- optimizador_mult_precond(f_rastrigin2d_vec,x0=c(0.8,4.8),eta=1)
contour(x1, x2, Z, las=1,
        xlab = expression(x[1]), ylab = expression(x[2]),
        main = "Función de Rastrigin",
        sub = "Curvas de nivel de la función")
lines(sol_ras_precon, type="b",cex=1.5,col="red")
lines(sol_ras, type="b",cex=1.5,col="blue")
legend("topright",col=c("red","blue"),legend = c("Con precondicionamiento","Sin precondicionamiento"),lty=1)
```

Ahora veamos que pasa cuando se utiliza la tasa de aprendizaje $\eta$:

```{r}
sol_ras_precon <- optimizador_mult_precond(f_rastrigin2d_vec,x0=c(0.8,4.8),eta=0.1)
contour(x1, x2, Z, las=1,
        xlab = expression(x[1]), ylab = expression(x[2]),
        main = "Función de Rastrigin",
        sub = "Curvas de nivel de la función")
lines(sol_ras_precon, type="b",cex=1.5,col="red")
lines(sol_ras, type="b",cex=1.5,col="blue")
legend("topright",col=c("red","blue"),legend = c("Con precondicionamiento","Sin precondicionamiento"),lty=1)
```

### Tasa de aprendizaje variable

La tasa de aprendizaje $\eta$ juega un papel importante en la optimización. Es ideal que al principio este parámetro permita movimientos de magnitud relevante, pero a medida que avanza el proceso dichos movimientos deben limitarse para evitar que el algoritmo diverja. Una forma de hacer esto es utilizar una tasa de aprendizaje variable como se muestra a continuación:

1.  $\eta_{(i+1)} = \eta_0 (ai+b)^{-c}$, $a,b,c>0$
2.  $\eta_{(i+1)} = \frac{\eta_{(i)}}{1+ai}$, con $a>0$
3.  $\eta_{(i+1)} = \eta_0 e^{-ai}$, $a>0$

En las fórmulas anteriores $\eta_0$ representa la tasa de aprendizaje del inicio del proceso y $\eta(i+1)$ es la tasa de aprendizaje de la iteración $i+1$.

### Gradiente estocástico

En aprendizaje de máquina con frecuencia las funciones objetivo toman la forma $E=\frac{1}{n} \sum_{k=1}^n{E_k}=\frac{1}{n} \sum_{k=1}^n{(Y_k- \hat{Y}_k)^2}$, donde $Y_k$ es la respuesta observada para el $k$-ésimo elemento de la muestra y $\hat{Y}_k$ es el valor predicho por el modelo. Generalmente $\hat{Y}_k=g(X_k,\omega)$, donde $g()$ es una función del vector de características del $k$-ésimo elemento de la muestra, $X_k$, y depende de un vector de parámetros $\omega$.

En el proceso de optimización se debe cacular $\nabla E=\sum_{k=1}^n{\nabla E_k}$. En lugar de calular $n$ gradientes para obtener $\nabla E$, se toma una muestra de observaciones de tamaño $N_{grad}$ sin reemplazo $i_1,i_2, \ldots,i_{N_{grad}}$ y se estima $\nabla E$ con $\nabla \hat E=\sum_{k=1}^{N_{grad}}{\nabla E_{i_k}}$.

Esto permite que la actualización de parámetros se haga usando $\nabla \hat E (w^{(i-1)})$.

### Minibatch

Un batch es un subconjunto de los datos de entrenamiento. En lugar de calcular el gradiente del error sobre todos los datos este se calcula sobre un batch. Así, los parámetros del modelo se actualizan usando la regla $\omega^{(i)}=\omega^{(i-1)} - \eta_{(i)} g^T_{(i)}(\omega^{(i-1)})$, donde $g_{(i)}=\frac{\partial}{\partial \omega} \frac{1}{|B_{(i)}|} \sum_{k \in B_{(i)}}{\nabla E_{i_k}}$, $B_{(i)}$ es el batch de observaciones escogidas al azar de la muestra en la iteración $i$ y $|B_{(i)}|$ es el número de elementos del batch.

### Momentum

A medida que el proceso de optimización avanza hacia un óptimo (local) el gradiente se desvanece. Esta información puede incorporarse en la regla de actualización de los parámetros del modelo. Para ello se define $v_{(i)}=\beta v_{(i-1)} + g_{(i)}$, con $\beta \in [0,1)$ y $v_{(0)}$ igual al vector cero. Así, la regla de actualización utilizando $v_{(i)}$ se escribe como $\omega^{(i)}=\omega^{(i-1)} - \eta_{(i)} v^T_{(i)}(\omega^{(i-1)})$.

Con $\beta=0$ este método equivale al gradiente estocástico en minibatch. El término $v_{(i)}$ permite que en la $i$-ésima actualización se use la información de las iteraciones previas.

### Adagrad

Definir la tasa de aprendizaje $\eta_{(i)}$ es un reto. Este parámetro afecta de manera importante el proceso de optimización. La regla de Adagrad (*adaptive gradient algorithm*) propone una tasa de aprendizaje para cada parámetro (entrada del vector $\omega$) que depende de la variación del gradiente con respecto al parámetro.

Se define $\eta_{(i)} = \eta G$, donde $G$ es una matriz diagonal cuyas entradas $G_{jj}$ son $\frac{1}{\sqrt{s_{(i),j}+\epsilon}}$, con $s_{(i),j}=s_{(i-1),j}+(g^{(j)}_{(i)})^2$, $g^{(j)}_{(i)}$ es la $j$-ésima componente del gradiente estimado en la iteración $i$ y $\epsilon>0$ es una pequeña constante que asegura que no haya división por cero.

### RMSProp

En Adagrad, el factor $s_{(i),j}$ puede crecer sin límites. Una alternativa para evitar esto es RMSProp (*Root Mean Square Propagation*) y consiste en modificar este factor así $s_{(i),j}=\gamma s_{(i-1),j}+(1-\gamma)(g^{(j)}_{(i)})^2$, con $\gamma \in [0,1]$.

### Adadelta

El método de Adadelta, como Adagram, usa la variación del gradiente. Adicionalmente incorpora la variación de los parámetros. Se define $\Delta \omega^{(i)}= \gamma \Delta \omega^{(i-1)} + (1- \gamma) ({g_e}^{(i)})^2$. En esta expresión $\Delta \omega^{(0)}$ se inicializa como un vector de ceros y ${g_e}^{(i)}$ es el gradiente estocástico escalado en la iteración $i$.

El gradiente se escala usando una matriz diagonal $G$ cuyas componentes son $G_{jj}=\frac{\sqrt{\Delta \omega^{(i-1),j}+ \epsilon}}{\sqrt{s_{(i),j}+\epsilon}}$. Aquí $s_{(i),j}$ se define como en RMSProp y $\Delta \omega^{(i-1),j}$ es la $j$-ésima componente del vector $\Delta \omega^{(i-1)}$.

### Adam

Este algoritmo incorpora las ideas de los algoritmos anteriores: gradiente estocástico, actualización de parámetros con información del gradiente durante todo el proceso, tasa de aprendizaje para cada componente y que depende de la variación del gradiente.

En este método se definen $v_{(i)}=\beta_1 v_{(i-1)} + (1-\beta_1) g_{(i)}$, $\beta_1>0$ y $s_{(i)}=\beta_2 s_{(i-1)} + (1-\beta_s) (g_{(i)})^2$, $\beta_2>0$. $v_{(0)}$ y $s_{(0)}$ se inicializan como un vector de ceros de $d$ componentes.

Luego se definen los componentes normalizados $\hat v_{(i)}=\frac{v_{(i)}}{1-\beta_1^t}$ y $\hat s_{(i)}=\frac{s_{(i)}}{1-\beta_2^t}$.

Ahora se define el gradiente escalado como ${g_e}^{(i)}$, un vector de $d$ componentes cuya $j$-ésima componente está dada por ${g_e}^{(i),j}=\frac{\eta \hat v_{(i)}}{\sqrt{\hat s_{(i)}}+\epsilon}$, con $\epsilon>0$ una pequeña constante que asegura la estabilidad del método.

La regla de actualización de parámetros es $\omega^{(i)}=\omega^{(i)} - g_e^{(i)}$.

## Consideraciones finales

La optimización numérica es una herramienta esencial para estimar o entrenar modelos de aprendizaje de máquina. Los métodos basados en gradiente ofrecen la ventaja de necesitar menos evaluaciones de la función objetivo, pero en contextos complejos, como no convexidad o ruido, pueden no converger. Una forma de superar estas limitaciones es incorprar elementos de adpatación como el precondicionamiento o la tasa de aprendizaje dinámica (depende del número de iteraciones) o que depende de la variación de los parámetros (los componentes del gradiente elevados al cuadrado).

Calibrar un modelo de aprendizaje de máquina en un contexto de alta dimensionalidad (muchos parámetros) o ruido puede requerir experimentación con los parámetros del optimizador. Saber qué es lo que mueven estos parámetros será de gran ayuda para el ingeniero de *machine learning* o para el científico de datos.

Fin
